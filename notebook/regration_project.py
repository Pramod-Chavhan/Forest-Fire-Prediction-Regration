# -*- coding: utf-8 -*-
"""Regration_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q2ujkbMVxoof_81d6tE8Hr0RYFHUReyB
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

df = pd.read_csv(r'dataset/Algerian_forest_fires_dataset.csv')

df.head()

df.info()

"""Data Cleaning

"""

df[df.isnull().any(axis=1)]

df.loc[:122,'Region'] = 0
df.loc[122:,'Region'] = 1

df.info()

df.head()

df.tail()

df[['Region']] = df[['Region']].astype(int)

df.head()

df.tail()

df.isnull().sum()

df = df.dropna().reset_index(drop=True)

df.columns

df.columns = df.columns.str.strip()

df.columns

df.info()

"""chnage the required column as integers

"""

df.columns

#  df[['day', 'month', 'year', 'Temperature', 'RH', 'Ws']] = df[['day', 'month', 'year', 'Temperature', 'RH', 'Ws']].astype(int)

# Inspect non-numeric data
for column in ['day', 'month', 'year', 'Temperature', 'RH', 'Ws']:
    df[column] = pd.to_numeric(df[column], errors='coerce')  # Convert to NaN if not numeric

# Drop rows with NaN values
df = df.dropna(subset=['day', 'month', 'year', 'Temperature', 'RH', 'Ws'])

# Convert to integers
df[['day', 'month', 'year', 'Temperature', 'RH', 'Ws']] = df[['day', 'month', 'year', 'Temperature', 'RH', 'Ws']].astype(int)

df.head()

df.info()

"""Changing the other Columns to float datatypes

"""

# Select columns with object data type
object_columns = [features for features in df.columns if df[features].dtype == 'object']

object_columns

for i in object_columns:
  if i !='Classes':
    df[i] =df[i].astype(float)

df.info()

df.describe()

df.head()

"""lets claen data set"""

df.to_csv('Algerian_forest_fires_dataset.csv',index=False)

"""EDA"""

df_copy = df.drop(['day','month','year'],axis=1)

df_copy.head()

df_copy.tail()

df_copy['Classes'].value_counts()

## Encoding of the category in classes
df_copy['Classes']= np.where(df_copy['Classes'].str.contains('not fire'),0,1)

df_copy.head()

df_copy.tail()

df_copy['Classes'].value_counts()

# PLot density plot for all features

# Seaborn Style: Use sns.set_style() for Seaborn-specific styles (whitegrid, darkgrid, white, dark, ticks).
#  Matplotlib Style: Use one from plt.style.available, such as 'ggplot', 'bmh', 'fast', 'seaborn-poster', etc.
sns.set_style('whitegrid')
df_copy.hist(bins = 50 , figsize=(20,15))
plt.show()

import matplotlib.pyplot as plt

# Calculate the percentage of each class
percentage = df_copy['Classes'].value_counts(normalize=True) * 100

# Dynamically generate class labels
classlabels = percentage.index.tolist()  # Get unique class names as labels

# Plotting the pie chart
plt.figure(figsize=(12, 7))
plt.pie(percentage, labels=classlabels, autopct='%1.1f%%')  # Create the pie chart
plt.title('Pie Chart of Classes')
plt.show()

"""Model Selection"""

df_copy

x = df_copy.iloc[:,:-2]

y = df_copy.iloc[:,-2]

x

y

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test =train_test_split(x,y, test_size=0.20, random_state=10)

x_train,y_train

x_test,y_test

# scalling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

x_test_scaled,x_train_scaled

#check Muliticolinerity

plt.Figure(figsize=(17,7))
corr = x_train.corr()
sns.heatmap(corr, annot=True)

x_train.corr()

def correlation (df_copy, threshold):
    col_corr = set()
    corr_matrix = df_copy.corr()
    for i in range (len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i,j]) > threshold:
                colname = corr_matrix.columns[i]
                col_corr.add(colname)
    return col_corr

# threshold - Domain Experties
corr_features = correlation(x_train,0.85)

corr_features

# drop feature when correlation is more than 0.85

x_train.drop(corr_features,axis=1,inplace=True)
x_test.drop(corr_features,axis=1,inplace=True)
x_train.shape,x_test.shape

# feature Scalling Or Standerdization

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

# box Plot to understand effect of Standerd Scaler

plt.subplots(figsize=(15,5))
plt.subplot(1,2,1)
sns.boxplot(data=x_train)
plt.title('X_train Before scalling')

plt.subplot(1,2,2)
sns.boxplot(data=x_train_scaled)
plt.title('X_train after Scalling')

# Lineare regression Model

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score,mean_absolute_error

linreg = LinearRegression()
linreg.fit(x_train_scaled,y_train)
y_pred = linreg.predict(x_test_scaled)
MAE = mean_absolute_error(y_test,y_pred)
SCORE = r2_score(y_test,y_pred)
print('mean absolute error : ',MAE)
print('r2_score : ',SCORE)

# Lasso Regression

from sklearn.linear_model import Lasso
from sklearn.metrics import r2_score,mean_absolute_error
lasso = Lasso()
lasso.fit(x_train_scaled,y_train)
y_pred = lasso.predict(x_test_scaled)
MAE = mean_absolute_error(y_test,y_pred)
SCORE = r2_score(y_test,y_pred)
print('mean absolute error : ',MAE)
print('r2_score : ',SCORE)

# Ridge Regression model

from sklearn.linear_model import Ridge
from sklearn.metrics import r2_score,mean_absolute_error
ridge = Ridge()
ridge.fit(x_train_scaled,y_train)
y_pred = ridge.predict(x_test_scaled)
MAE = mean_absolute_error(y_test,y_pred)
SCORE = r2_score(y_test,y_pred)
print('mean absolute error : ',MAE)
print('r2_score : ',SCORE)

x_train_scaled

# ElasticNet Regression Model

from sklearn.linear_model import ElasticNet
from sklearn.metrics import r2_score,mean_absolute_error
elastic = ElasticNet()
elastic.fit(x_train_scaled,y_train)
y_pred = elastic.predict(x_test_scaled)
MAE = mean_absolute_error(y_test,y_pred)
SCORE = r2_score(y_test,y_pred)
print('mean absolute error : ',MAE)
print('r2_score : ',SCORE)

# Pickling

import pickle
pickle.dump(scaler,open('scaler.pkl','wb'))
pickle.dump(ridge,open('ridge.pkl','wb'))

